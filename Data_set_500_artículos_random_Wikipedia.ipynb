{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaC6UWeJ6Q0xVcqHR1JeaE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cZantana/Dataset-random-wikipedia/blob/main/Data_set_500_art%C3%ADculos_random_Wikipedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8Z5Hv5M_WQYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw0iubsVWAU8"
      },
      "outputs": [],
      "source": [
        "titulo = []\n",
        "articu = []\n",
        "\n",
        "for i in range(0,500):\n",
        "\n",
        "  wikirandom='https://es.wikipedia.org/wiki/Special:Random'\n",
        "\n",
        "  urlrand = requests.get(wikirandom)\n",
        "\n",
        "  charini=urlrand.text.find('\"wgPageName\":\"')\n",
        "  charfin=urlrand.text.find('\",\"wgTitle\":')\n",
        "\n",
        "  articulo=urlrand.text[charini+14:charfin]\n",
        "\n",
        "  if articulo[0]=='\"':\n",
        "    articulo=articulo[1:]\n",
        "  \n",
        "  if articulo[-1]==',':\n",
        "    print(\"COMAAAA\")\n",
        "    \n",
        "  linkjson='https://es.wikipedia.org/w/api.php?action=parse&format=json&page='+articulo+'&prop=text&formatversion=2'\n",
        "  print(i,articulo,\"\\n\")\n",
        "\n",
        "  try:\n",
        "    url = requests.get(linkjson)\n",
        "\n",
        "    newurljson = json.loads(url.text)\n",
        "\n",
        "    tbody = re.compile('<tbody><tr>(.|\\n|\\n\\n)*?</tr></tbody>')\n",
        "\n",
        "    encpat = tbody.finditer(str(newurljson['parse']['text']))\n",
        "\n",
        "    chaon = re.compile('\\n')\n",
        "\n",
        "    tabla = re.compile('<td.*?>')\n",
        "\n",
        "    tabla2 = re.compile('<th.*?>')\n",
        "\n",
        "    clean = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "\n",
        "    for noce in encpat:\n",
        "      #print(\"noce de cero \\n\\n\",noce[0])\n",
        "      #print(noce.regs[0][0],noce.regs[0][1])\n",
        "      #print(type(newurljson['parse']['text'][noce.regs[0][0]:noce.regs[0][1]]))\n",
        "      tbodyclean = re.sub(chaon,'',noce[0])\n",
        "      tbodyclean = re.sub(tabla,':\\t',tbodyclean)\n",
        "      tbodyclean = re.sub(tabla2,'\\n',tbodyclean)\n",
        "      tbodyclean = re.sub(clean,'',tbodyclean)\n",
        "      newurljson['parse']['text']=newurljson['parse']['text'].replace(noce[0],tbodyclean)\n",
        "      #print(\"\\n\\nCambiado\\n\\n\")\n",
        "      #print(tbodyclean)\n",
        "      #print(newurljson['parse']['text'])\n",
        "\n",
        "    clean = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "\n",
        "    htmltext=re.sub(clean, '', newurljson['parse']['text'])\n",
        "\n",
        "    titulo.append(articulo)\n",
        "    articu.append(htmltext)\n",
        "  except:\n",
        "    print(\"Error:\",Exception)\n",
        "dataset = pd.DataFrame(list(zip(titulo,articu)),columns=['Titulo','Articulo'])\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.to_csv('/content/drive/MyDrive/Proyecto Trabajo NLP IA/Articulos.csv', index=True, header=True)"
      ],
      "metadata": {
        "id": "x7ENVH9YWHph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abrirdataset=pd.read_csv(\"/content/drive/MyDrive/Proyecto Trabajo NLP IA/Articulos.csv\",index_col=0)\n",
        "\n",
        "datanumpy=abrirdataset.to_numpy()\n",
        "\n",
        "for i in range(len(datanumpy)):\n",
        "  print(i)\n",
        "  numarti=i\n",
        "  titulo = datanumpy[numarti][0].replace('_',' ')\n",
        "\n",
        "  #print(datanumpy[numarti][0],\"\\n\")\n",
        "  #print(titulo)\n",
        "  #print(datanumpy[numarti][1])\n",
        "  #print('\\n\\n\\n\\n--------------------------------------\\n\\n\\n\\n')\n",
        "  patron = re.compile(fr'.mw-parser-output..*|-->|<!--|↑|\\[editar]|\\[+[0-9]+\\]|\\t|Control de autoridades:|Índice(.|\\n)*?\\n\\n\\n\\n|(.|\\n)*?editar datos en Wikidata|(Enlaces externos|Proyectos Wikimedia|Datos: Q2|Referencias|Véase también)(.|\\n)*')\n",
        "\n",
        "  encpat = patron.finditer(str(datanumpy[numarti][1]))\n",
        "\n",
        "  datanumpy[numarti][1]=re.sub(patron, '', datanumpy[numarti][1])\n",
        "\n",
        "  '''#print(\"\\n\\n\\n\\nCANTIDAD DE MATCHES:\")\n",
        "  for noce in encpat:\n",
        "    #print(noce[0])\n",
        "    #print(noce.regs[0][0],noce.regs[0][1])\n",
        "    datanumpy[numarti][1]=datanumpy[numarti][1].replace(str(noce[0]),'')'''\n",
        "\n",
        "  ponertitulo = re.compile(fr'^')\n",
        "  datanumpy[numarti][1]=re.sub(ponertitulo, titulo+'\\n\\n', datanumpy[numarti][1])\n",
        "  #print('------------------------------------------------------------------------------------------------------------------------------------')\n",
        "  #print(datanumpy[numarti][1])\n",
        "  #print('------------------------------------------------------------------------------------------------------------------------------------')\n",
        "  #PARRAFOS:\n",
        "\n",
        "  parrafos = re.compile(fr'[A-Z](.|\\n)*?(\\n\\n|$)')\n",
        "\n",
        "  encparr = parrafos.finditer(str(datanumpy[numarti][1]))  \n",
        "\n",
        "  #print(datanumpy[numarti].shape)\n",
        "  \n",
        "  #print(datanumpy[numarti].shape)\n",
        "  #print(datanumpy[numarti])\n",
        "  datanumpy=datanumpy.tolist()\n",
        "  #print('entra for')\n",
        "  for noce in encparr:\n",
        "    #print('------------------------------------------------------------------------------------------------------------------------------------')    \n",
        "    #print(noce[0])\n",
        "    #print('------------------------------------------------------------------------------------------------------------------------------------')\n",
        "    #print(noce.regs[0][0],noce.regs[0][1])\n",
        "    datanumpy[numarti].append(noce[0])\n",
        "    #print(datanumpy[numarti])\n",
        "    #print(len(datanumpy[numarti]))\n",
        "  #print('salefor')\n",
        "  datanumpy=np.array(datanumpy)\n",
        "  #print(datanumpy[numarti])\n",
        "\n",
        "#print(datanumpy[0:1])\n",
        "\n",
        "#print(datanumpy[numarti][1])\n",
        "\n",
        "print(len(max(datanumpy, key=len)))\n",
        "columnas=['Titulo','Articulo']\n",
        "for i in range(len(max(datanumpy, key=len))-2):\n",
        "  columnas.append('Parrafo '+str(i+1))\n",
        "\n",
        "datasetnew=pd.DataFrame(list(datanumpy),columns=columnas)\n",
        "\n",
        "datasetnew.to_csv('/content/drive/MyDrive/Proyecto Trabajo NLP IA/Parrafos.csv', index=True, header=True)\n",
        "\n",
        "datasetnew"
      ],
      "metadata": {
        "id": "J8KmUguzWKtg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}